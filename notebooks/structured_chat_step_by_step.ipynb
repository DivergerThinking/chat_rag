{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import langchain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import Tool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/secrets_itor.json\") as secrets:\n",
    "    secrets_dict = eval(secrets.read())\n",
    "    open_api_key = base64.b64decode(secrets_dict[\"openai_api_key\"]).decode('ascii')\n",
    "    os.environ[\"OPENAI_API_KEY\"] = open_api_key\n",
    "    if \"organization_id\" in secrets_dict.keys():\n",
    "        openai_organization = base64.b64decode(secrets_dict[\"organization_id\"]).decode('ascii')\n",
    "        os.environ[\"OPENAI_ORGANIZATION\"] = openai_organization\n",
    "\n",
    "from chatrag.retriever import create_retriever_from_csv, create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\", max_tokens=2000)\n",
    "# retriever_llm = ChatOpenAI(temperature=0, model=\"gpt-4\", max_tokens=2000)\n",
    "retriever = create_retriever_from_csv(\n",
    "    csv_path=\"../data/movies_title_overview_vote.csv\",\n",
    "    metadata_columns_dtypes={\"monthly_traffic\": \"int\"},\n",
    "    llm=retriever_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorstore DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain_llm = ChatOpenAI(temperature=0, model=\"gpt-4\", max_tokens=1000)\n",
    "# retrieval_chain_llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\", max_tokens=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_media_sq_retrieval_chain = create_retrieval_chain(retriever=retriever, llm=retrieval_chain_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_tool = load_tools([\"llm-math\"], llm=OpenAI(temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_description = \"\"\"Movie search tool. The action input must be just topics in a natural language sentence\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Search movies\",\n",
    "        func=g_media_sq_retrieval_chain.run,\n",
    "        description=retriever_description\n",
    "    ),\n",
    "    calc_tool[0]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\" You are an assistant expert in movies recommendation. You should guide and help the user through the whole process until suggesting the best movie options to watch. You should attend to all the user requirements always taking into account user data. \n",
    "\n",
    "For the first interactions you should collect some user configuration data. This data will restrict the movies to consider.\n",
    "\n",
    "User Data to collect (mandatory):\n",
    "    Target genre: Movie genre e.g. fiction, adventure, trhiller...\n",
    "    Movie overview topic: List of keywords defining the campaign context.\n",
    "\n",
    "After succesfully collecting data, you should keep the conversation with the human, answering the questions and requests as good as you can. To do so, you have access to the following tools:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"\"\"Begin! Your first action must ve collect user data and keep it. Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB``` then Observation: ... Thought: ... Action: ...\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = MessagesPlaceholder(variable_name=\"chat_history\")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "agent_kwargs = {\n",
    "        \"memory_prompts\": [chat_history],\n",
    "        \"input_variables\": [\"input\", \"agent_scratchpad\", \"chat_history\"],\n",
    "        \"prefix\": prefix,\n",
    "        \"suffix\": suffix\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_llm_model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\", max_tokens=1000)\n",
    "agent_llm_model = ChatOpenAI(temperature=0, model=\"gpt-4\", max_tokens=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_chat_agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=agent_llm_model,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    agent_kwargs=agent_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stepwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union\n",
    "from langchain.utils.input import get_color_mapping\n",
    "from langchain.schema import (\n",
    "    AgentAction,\n",
    "    AgentFinish)\n",
    "import time\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "def _call_stepwise(\n",
    "        self,\n",
    "        inputs: Dict[str, str],\n",
    "        run_manager = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Run text through and get agent response.\"\"\"\n",
    "        # Construct a mapping of tool name to tool for easy lookup\n",
    "        name_to_tool_map = {tool.name: tool for tool in self.tools}\n",
    "        # We construct a mapping from each tool to a color, used for logging.\n",
    "        color_mapping = get_color_mapping(\n",
    "            [tool.name for tool in self.tools], excluded_colors=[\"green\", \"red\"]\n",
    "        )\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]] = []\n",
    "        # Let's start tracking the number of iterations and time elapsed\n",
    "        iterations = 0\n",
    "        time_elapsed = 0.0\n",
    "        start_time = time.time()\n",
    "        # We now enter the agent loop (until it returns something).\n",
    "        while self._should_continue(iterations, time_elapsed):\n",
    "            set_trace()\n",
    "            next_step_output = self._take_next_step(\n",
    "                name_to_tool_map,\n",
    "                color_mapping,\n",
    "                inputs,\n",
    "                intermediate_steps,\n",
    "                run_manager=run_manager,\n",
    "            )\n",
    "            if isinstance(next_step_output, AgentFinish):\n",
    "                return self._return(\n",
    "                    next_step_output, intermediate_steps, run_manager=run_manager\n",
    "                )\n",
    "\n",
    "            intermediate_steps.extend(next_step_output)\n",
    "            if len(next_step_output) == 1:\n",
    "                next_step_action = next_step_output[0]\n",
    "                # See if tool should return directly\n",
    "                tool_return = self._get_tool_return(next_step_action)\n",
    "                if tool_return is not None:\n",
    "                    return self._return(\n",
    "                        tool_return, intermediate_steps, run_manager=run_manager\n",
    "                    )\n",
    "            iterations += 1\n",
    "            time_elapsed = time.time() - start_time\n",
    "        output = self.agent.return_stopped_response(\n",
    "            self.early_stopping_method, intermediate_steps, **inputs\n",
    "        )\n",
    "        return self._return(output, intermediate_steps, run_manager=run_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = general_chat_agent.prep_inputs(\"I want to watch a movie about outer space exploration.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a mapping of tool name to tool for easy lookup\n",
    "name_to_tool_map = {tool.name: tool for tool in general_chat_agent.tools}\n",
    "# We construct a mapping from each tool to a color, used for logging.\n",
    "color_mapping = get_color_mapping(\n",
    "    [tool.name for tool in general_chat_agent.tools], excluded_colors=[\"green\", \"red\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_steps: List[Tuple[AgentAction, str]] = []\n",
    "# Let's start tracking the number of iterations and time elapsed\n",
    "iterations = 0\n",
    "time_elapsed = 0.0\n",
    "# start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "next_step_output = general_chat_agent._take_next_step(\n",
    "    name_to_tool_map,\n",
    "    color_mapping,\n",
    "    inputs,\n",
    "    intermediate_steps,\n",
    "    run_manager=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_step_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next_step_output.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_step_output.return_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(next_step_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(next_step_output, AgentFinish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_chat_agent.prep_outputs(inputs, next_step_output.return_values, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_chat_agent.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = general_chat_agent.prep_inputs(\"Thriller.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a mapping of tool name to tool for easy lookup\n",
    "name_to_tool_map = {tool.name: tool for tool in general_chat_agent.tools}\n",
    "# We construct a mapping from each tool to a color, used for logging.\n",
    "color_mapping = get_color_mapping(\n",
    "    [tool.name for tool in general_chat_agent.tools], excluded_colors=[\"green\", \"red\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_steps: List[Tuple[AgentAction, str]] = []\n",
    "# Let's start tracking the number of iterations and time elapsed\n",
    "iterations = 0\n",
    "time_elapsed = 0.0\n",
    "# start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_step_output = general_chat_agent._take_next_step(\n",
    "    name_to_tool_map,\n",
    "    color_mapping,\n",
    "    inputs,\n",
    "    intermediate_steps,\n",
    "    run_manager=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(next_step_output, AgentFinish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_step_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_step_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(next_step_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_step_output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next_step_output[0][0].log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_step_output[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_steps.extend(next_step_output)\n",
    "intermediate_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(next_step_output) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_step_action = next_step_output[0]\n",
    "next_step_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_return = general_chat_agent._get_tool_return(next_step_action)\n",
    "print(tool_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{tool.name: tool for tool in general_chat_agent.tools}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_step_action[0].tool in name_to_tool_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_tool_map[next_step_action[0].tool].return_direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations += 1\n",
    "time_elapsed = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_chat_agent.agent.get_full_inputs(intermediate_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(general_chat_agent.agent.get_full_inputs(intermediate_steps)[\"agent_scratchpad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_step_output = general_chat_agent._take_next_step(\n",
    "    name_to_tool_map,\n",
    "    color_mapping,\n",
    "    inputs,\n",
    "    intermediate_steps,\n",
    "    run_manager=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_step_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(next_step_output, AgentFinish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next_step_output.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_chat_agent._return(next_step_output, intermediate_steps, run_manager=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce plan step\n",
    "With tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan(\n",
    "    self,\n",
    "    intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "    callbacks: Callbacks = None,\n",
    "    **kwargs: Any,\n",
    ") -> Union[AgentAction, AgentFinish]:\n",
    "    \"\"\"Given input, decided what to do.\n",
    "\n",
    "    Args:\n",
    "        intermediate_steps: Steps the LLM has taken to date,\n",
    "            along with observations\n",
    "        callbacks: Callbacks to run.\n",
    "        **kwargs: User inputs.\n",
    "\n",
    "    Returns:\n",
    "        Action specifying what tool to use.\n",
    "    \"\"\"\n",
    "    full_inputs = self.get_full_inputs(intermediate_steps, **kwargs)\n",
    "    full_output = self.llm_chain.predict(callbacks=callbacks, **full_inputs)\n",
    "    return self.output_parser.parse(full_output)\n",
    "\n",
    "def predict(self, callbacks: Callbacks = None, **kwargs: Any) -> str:\n",
    "    \"\"\"Format prompt with kwargs and pass to LLM.\n",
    "\n",
    "    Args:\n",
    "        callbacks: Callbacks to pass to LLMChain\n",
    "        **kwargs: Keys to pass to prompt template.\n",
    "\n",
    "    Returns:\n",
    "        Completion from LLM.\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            completion = llm.predict(adjective=\"funny\")\n",
    "    \"\"\"\n",
    "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
    "\n",
    "def _call(\n",
    "    self,\n",
    "    inputs: Dict[str, Any],\n",
    "    run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    ") -> Dict[str, str]:\n",
    "    response = self.generate([inputs], run_manager=run_manager)\n",
    "    return self.create_outputs(response)[0]\n",
    "\n",
    "def generate(\n",
    "    self,\n",
    "    input_list: List[Dict[str, Any]],\n",
    "    run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    ") -> LLMResult:\n",
    "    \"\"\"Generate LLM result from inputs.\"\"\"\n",
    "    prompts, stop = self.prep_prompts(input_list, run_manager=run_manager)\n",
    "    return self.llm.generate_prompt(\n",
    "        prompts,\n",
    "        stop,\n",
    "        callbacks=run_manager.get_child() if run_manager else None,\n",
    "        **self.llm_kwargs,\n",
    "    )\n",
    "\n",
    "def generate_prompt(\n",
    "    self,\n",
    "    prompts: List[PromptValue],\n",
    "    stop: Optional[List[str]] = None,\n",
    "    callbacks: Callbacks = None,\n",
    "    **kwargs: Any,\n",
    ") -> LLMResult:\n",
    "    prompt_messages = [p.to_messages() for p in prompts]\n",
    "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
    "\n",
    "\n",
    "def generate(\n",
    "    self,\n",
    "    messages: List[List[BaseMessage]],\n",
    "    stop: Optional[List[str]] = None,\n",
    "    callbacks: Callbacks = None,\n",
    "    *,\n",
    "    tags: Optional[List[str]] = None,\n",
    "    metadata: Optional[Dict[str, Any]] = None,\n",
    "    **kwargs: Any,\n",
    ") -> LLMResult:\n",
    "    \"\"\"Top Level call\"\"\"\n",
    "    params = self._get_invocation_params(stop=stop, **kwargs)\n",
    "    options = {\"stop\": stop}\n",
    "\n",
    "    callback_manager = CallbackManager.configure(\n",
    "        callbacks,\n",
    "        self.callbacks,\n",
    "        self.verbose,\n",
    "        tags,\n",
    "        self.tags,\n",
    "        metadata,\n",
    "        self.metadata,\n",
    "    )\n",
    "    run_managers = callback_manager.on_chat_model_start(\n",
    "        dumpd(self), messages, invocation_params=params, options=options\n",
    "    )\n",
    "    results = []\n",
    "    for i, m in enumerate(messages):\n",
    "        try:\n",
    "            results.append(\n",
    "                self._generate_with_cache(\n",
    "                    m,\n",
    "                    stop=stop,\n",
    "                    run_manager=run_managers[i] if run_managers else None,\n",
    "                    **kwargs,\n",
    "                )\n",
    "            )\n",
    "        except (KeyboardInterrupt, Exception) as e:\n",
    "            if run_managers:\n",
    "                run_managers[i].on_llm_error(e)\n",
    "            raise e\n",
    "    flattened_outputs = [\n",
    "        LLMResult(generations=[res.generations], llm_output=res.llm_output)\n",
    "        for res in results\n",
    "    ]\n",
    "    llm_output = self._combine_llm_outputs([res.llm_output for res in results])\n",
    "    generations = [res.generations for res in results]\n",
    "    output = LLMResult(generations=generations, llm_output=llm_output)\n",
    "    if run_managers:\n",
    "        run_infos = []\n",
    "        for manager, flattened_output in zip(run_managers, flattened_outputs):\n",
    "            manager.on_llm_end(flattened_output)\n",
    "            run_infos.append(RunInfo(run_id=manager.run_id))\n",
    "        output.run = run_infos\n",
    "    return output\n",
    "\n",
    "\n",
    "def _generate(\n",
    "    self,\n",
    "    messages: List[BaseMessage],\n",
    "    stop: Optional[List[str]] = None,\n",
    "    run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "    stream: Optional[bool] = None,\n",
    "    **kwargs: Any,\n",
    ") -> ChatResult:\n",
    "    if stream if stream is not None else self.streaming:\n",
    "        generation: Optional[ChatGenerationChunk] = None\n",
    "        for chunk in self._stream(\n",
    "            messages=messages, stop=stop, run_manager=run_manager, **kwargs\n",
    "        ):\n",
    "            if generation is None:\n",
    "                generation = chunk\n",
    "            else:\n",
    "                generation += chunk\n",
    "        assert generation is not None\n",
    "        return ChatResult(generations=[generation])\n",
    "\n",
    "    message_dicts, params = self._create_message_dicts(messages, stop)\n",
    "    params = {**params, **kwargs}\n",
    "    response = self.completion_with_retry(\n",
    "        messages=message_dicts, run_manager=run_manager, **params\n",
    "    )\n",
    "    return self._create_chat_result(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_inputs = general_chat_agent.agent.get_full_inputs([], **inputs)\n",
    "full_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_output = general_chat_agent.agent.llm_chain.predict(callbacks=None, **full_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_chat_agent.agent.output_parser.parse(full_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"System:  You are an assistant expert in movies recommendation. You should guide and help the user through the whole process until suggesting the best movie options to watch. You should attend to all the user requirements always taking into account user data. \\n\\nFor the first interactions you should collect some user configuration data. This data will restrict the movies to consider.\\n\\nUser Data to collect (mandatory):\\n    Target genre: Movie genre e.g. fiction, adventure, trhiller...\\n    Movie overview topic: List of keywords defining the campaign context.\\n\\nAfter succesfully collecting data, you should keep the conversation with the human, answering the questions and requests as good as you can. To do so, you have access to the following tools:\\n\\nSearch movies: Movie search tool. The action input must be just topics in a natural language sentence, args: {{'tool_input': {{'type': 'string'}}}}\\nCalculator: Useful for when you need to answer questions about math., args: {{'tool_input': {{'type': 'string'}}}}\\n\\nUse a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\\n\\nValid \\\"action\\\" values: \\\"Final Answer\\\" or Search movies, Calculator\\n\\nProvide only ONE action per $JSON_BLOB, as shown:\\n\\n```\\n{\\n  \\\"action\\\": $TOOL_NAME,\\n  \\\"action_input\\\": $INPUT\\n}\\n```\\n\\nFollow this format:\\n\\nQuestion: input question to answer\\nThought: consider previous and subsequent steps\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: action result\\n... (repeat Thought/Action/Observation N times)\\nThought: I know what to respond\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"Final Answer\\\",\\n  \\\"action_input\\\": \\\"Final response to human\\\"\\n}\\n```\\n\\nBegin! Your first action must ve collect user data and keep it. Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB``` then Observation: ... Thought: ... Action: ...\\nHuman: I want to watch a movie about outer space exploration.\\nAI: Sure, I can help with that. Could you please specify your preferred genre? For example, are you interested in science fiction, adventure, drama, or something else?\\nHuman: Thriller.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts, stop = general_chat_agent.agent.llm_chain.prep_prompts([full_inputs])\n",
    "prompts, stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general_chat_agent.agent.generate(\n",
    "#         [prompt],\n",
    "#         stop=stop,\n",
    "#         callbacks=callbacks,\n",
    "#         tags=tags,\n",
    "#         metadata=metadata,\n",
    "#         **kwargs,\n",
    "#     ).generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts[0].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_chat_agent.agent.llm_chain.llm.generate_prompt(\n",
    "        prompts,\n",
    "        stop,\n",
    "        callbacks=None,\n",
    "        **general_chat_agent.agent.llm_chain.llm_kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_messages = [p.to_messages() for p in prompts]\n",
    "prompt_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_messages = [p.to_messages() for p in prompts]\n",
    "prompt_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(general_chat_agent.agent.llm_chain.llm.cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(langchain.llm_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, m in enumerate(prompt_messages):\n",
    "    print(i)\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_messages[0][3].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_chat_agent.agent.llm_chain.llm._generate(prompt_messages[0], stop=stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_dicts, params = general_chat_agent.agent.llm_chain.llm._create_message_dicts(prompt_messages[0], stop)\n",
    "message_dicts, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['top_p'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = general_chat_agent.agent.llm_chain.llm.completion_with_retry(messages=message_dicts, **params)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = general_chat_agent.agent.llm_chain.llm.completion_with_retry(messages=message_dicts, **params)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_results = general_chat_agent.agent.llm_chain.llm._create_chat_result(response)\n",
    "chat_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import LLMResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_outputs = [LLMResult(generations=[res.generations], llm_output=res.llm_output) for res in [chat_results]]\n",
    "flattened_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_output = general_chat_agent.agent.llm_chain.llm._combine_llm_outputs([res.llm_output for res in [chat_results]])\n",
    "generations = [res.generations for res in [chat_results]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_output, generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_output = LLMResult(generations=generations, llm_output=llm_output)\n",
    "generate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_output_inner = general_chat_agent.agent.llm_chain.create_outputs(generate_output)[0]\n",
    "full_output_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_chat_agent.agent.output_parser.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porqué el parse ejecuta una LLM chain? Pensaba que era un StructuredChatOutputParserWithRetries y este no tiene nada de eso (creo)\n",
    "output_parser_out = general_chat_agent.agent.output_parser.parse(full_output_inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_pip_github",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
